# Tips & Tricks for Analysis/Algebra in Machine learning
Random assortment of some usefull tricks I have encountered that can be hard to get correct.

## Matrix Gradients
[looks **VERY** helpful](https://tminka.github.io/papers/matrix/minka-matrix.pdf)
[gradients w.r.t. matrix](http://thousandfold.net/cz/2013/11/12/a-useful-trick-for-computing-gradients-w-r-t-matrix-arguments-with-some-examples/)
[matrix recipes](http://mplab.ucsd.edu/tutorials/MatrixRecipes.pdf)


## Linear Algebra
### Schur
Schur Complement of square A is (n-1) square matrix. If A is invertible, it is too.
Consult CLRS in the LU factoring chapter.
        Murphy in the regression chapter.

## Convexity
### Partition Function -like
log sum exp

## Exponential Families
forward
backward

## Common Functions
sigmoid' = sigmoid * (1-sigmoid)
